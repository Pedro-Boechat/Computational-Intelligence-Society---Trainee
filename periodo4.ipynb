{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pedro-Boechat/trainee/blob/main/periodo4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ovfMkx-2A1tp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejQR1RV99vmz"
      },
      "source": [
        "# Binário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NScRg3I6Dx1z"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/creditcard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "rdW6kMv5EDnr",
        "outputId": "295352b5-70cf-4dd3-e360-dab02659687e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e8ae57fc-a98e-4d1f-90b3-27af4cfef190\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8ae57fc-a98e-4d1f-90b3-27af4cfef190')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e8ae57fc-a98e-4d1f-90b3-27af4cfef190 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e8ae57fc-a98e-4d1f-90b3-27af4cfef190');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0     0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62    0.0\n",
              "1     0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69    0.0\n",
              "2     1 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66    0.0\n",
              "3     1 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50    0.0\n",
              "4     2 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99    0.0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDQVNrqED3Er"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "d8CcGzySDcAD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjmTzq1qlare",
        "outputId": "26187fab-4482-497d-8015-a64c3c84f4b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    81100\n",
              "1.0      198\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pBnjhKngElaz"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns='Time')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "xRVWGRSiXmxx"
      },
      "outputs": [],
      "source": [
        "colunas = df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOlx4MSfEhX7"
      },
      "source": [
        "## Normalizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "c-WByjVLEH26"
      },
      "outputs": [],
      "source": [
        "for i in colunas:\n",
        "  df[i] = (df[i] - df[i].min()) / (df[i].max() - df[i].min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "zern-3TSXing",
        "outputId": "4dd827c3-a6dc-4cc4-9d1e-938dc23ee332"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-83c33198-b8ad-43e2-811e-f3270087982d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.762583</td>\n",
              "      <td>0.660414</td>\n",
              "      <td>0.909721</td>\n",
              "      <td>0.565623</td>\n",
              "      <td>0.798543</td>\n",
              "      <td>0.159124</td>\n",
              "      <td>0.131668</td>\n",
              "      <td>0.765803</td>\n",
              "      <td>0.363363</td>\n",
              "      <td>0.236216</td>\n",
              "      <td>0.308548</td>\n",
              "      <td>0.437774</td>\n",
              "      <td>0.364897</td>\n",
              "      <td>0.732471</td>\n",
              "      <td>0.701250</td>\n",
              "      <td>0.405258</td>\n",
              "      <td>0.597465</td>\n",
              "      <td>0.595135</td>\n",
              "      <td>0.604822</td>\n",
              "      <td>0.615614</td>\n",
              "      <td>0.408828</td>\n",
              "      <td>0.645188</td>\n",
              "      <td>0.481783</td>\n",
              "      <td>0.660030</td>\n",
              "      <td>0.531926</td>\n",
              "      <td>0.224086</td>\n",
              "      <td>0.595303</td>\n",
              "      <td>0.394042</td>\n",
              "      <td>0.019400</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.961527</td>\n",
              "      <td>0.679003</td>\n",
              "      <td>0.765280</td>\n",
              "      <td>0.478470</td>\n",
              "      <td>0.808561</td>\n",
              "      <td>0.137239</td>\n",
              "      <td>0.123551</td>\n",
              "      <td>0.764960</td>\n",
              "      <td>0.298603</td>\n",
              "      <td>0.219554</td>\n",
              "      <td>0.660360</td>\n",
              "      <td>0.760630</td>\n",
              "      <td>0.590167</td>\n",
              "      <td>0.752040</td>\n",
              "      <td>0.586455</td>\n",
              "      <td>0.527165</td>\n",
              "      <td>0.563077</td>\n",
              "      <td>0.563349</td>\n",
              "      <td>0.524169</td>\n",
              "      <td>0.599172</td>\n",
              "      <td>0.390749</td>\n",
              "      <td>0.451615</td>\n",
              "      <td>0.507877</td>\n",
              "      <td>0.539604</td>\n",
              "      <td>0.543972</td>\n",
              "      <td>0.291007</td>\n",
              "      <td>0.579790</td>\n",
              "      <td>0.399230</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.762696</td>\n",
              "      <td>0.590904</td>\n",
              "      <td>0.863209</td>\n",
              "      <td>0.472062</td>\n",
              "      <td>0.794397</td>\n",
              "      <td>0.212882</td>\n",
              "      <td>0.145736</td>\n",
              "      <td>0.775036</td>\n",
              "      <td>0.166905</td>\n",
              "      <td>0.243769</td>\n",
              "      <td>0.499724</td>\n",
              "      <td>0.568963</td>\n",
              "      <td>0.624889</td>\n",
              "      <td>0.749448</td>\n",
              "      <td>0.822259</td>\n",
              "      <td>0.089544</td>\n",
              "      <td>0.693562</td>\n",
              "      <td>0.572772</td>\n",
              "      <td>0.213737</td>\n",
              "      <td>0.629648</td>\n",
              "      <td>0.432034</td>\n",
              "      <td>0.749491</td>\n",
              "      <td>0.607457</td>\n",
              "      <td>0.436154</td>\n",
              "      <td>0.389684</td>\n",
              "      <td>0.234712</td>\n",
              "      <td>0.574744</td>\n",
              "      <td>0.388431</td>\n",
              "      <td>0.049097</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.793265</td>\n",
              "      <td>0.654247</td>\n",
              "      <td>0.864414</td>\n",
              "      <td>0.355570</td>\n",
              "      <td>0.806792</td>\n",
              "      <td>0.190653</td>\n",
              "      <td>0.131617</td>\n",
              "      <td>0.783077</td>\n",
              "      <td>0.180253</td>\n",
              "      <td>0.226795</td>\n",
              "      <td>0.361395</td>\n",
              "      <td>0.590476</td>\n",
              "      <td>0.593006</td>\n",
              "      <td>0.735188</td>\n",
              "      <td>0.411774</td>\n",
              "      <td>0.328374</td>\n",
              "      <td>0.502426</td>\n",
              "      <td>0.889970</td>\n",
              "      <td>0.364728</td>\n",
              "      <td>0.592043</td>\n",
              "      <td>0.400986</td>\n",
              "      <td>0.587621</td>\n",
              "      <td>0.471944</td>\n",
              "      <td>0.292186</td>\n",
              "      <td>0.693705</td>\n",
              "      <td>0.217115</td>\n",
              "      <td>0.587594</td>\n",
              "      <td>0.406007</td>\n",
              "      <td>0.016013</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.778299</td>\n",
              "      <td>0.712545</td>\n",
              "      <td>0.849526</td>\n",
              "      <td>0.474241</td>\n",
              "      <td>0.796811</td>\n",
              "      <td>0.144401</td>\n",
              "      <td>0.140675</td>\n",
              "      <td>0.742920</td>\n",
              "      <td>0.410841</td>\n",
              "      <td>0.279026</td>\n",
              "      <td>0.264458</td>\n",
              "      <td>0.659528</td>\n",
              "      <td>0.720530</td>\n",
              "      <td>0.637955</td>\n",
              "      <td>0.522974</td>\n",
              "      <td>0.407730</td>\n",
              "      <td>0.550055</td>\n",
              "      <td>0.585411</td>\n",
              "      <td>0.663428</td>\n",
              "      <td>0.623674</td>\n",
              "      <td>0.409601</td>\n",
              "      <td>0.755109</td>\n",
              "      <td>0.478458</td>\n",
              "      <td>0.682038</td>\n",
              "      <td>0.427610</td>\n",
              "      <td>0.370969</td>\n",
              "      <td>0.604648</td>\n",
              "      <td>0.428293</td>\n",
              "      <td>0.009075</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83c33198-b8ad-43e2-811e-f3270087982d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83c33198-b8ad-43e2-811e-f3270087982d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83c33198-b8ad-43e2-811e-f3270087982d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         V1        V2        V3        V4  ...       V27       V28    Amount  Class\n",
              "0  0.762583  0.660414  0.909721  0.565623  ...  0.595303  0.394042  0.019400    0.0\n",
              "1  0.961527  0.679003  0.765280  0.478470  ...  0.579790  0.399230  0.000349    0.0\n",
              "2  0.762696  0.590904  0.863209  0.472062  ...  0.574744  0.388431  0.049097    0.0\n",
              "3  0.793265  0.654247  0.864414  0.355570  ...  0.587594  0.406007  0.016013    0.0\n",
              "4  0.778299  0.712545  0.849526  0.474241  ...  0.604648  0.428293  0.009075    0.0\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_OcpY8vGzfj"
      },
      "source": [
        "## Verificar balanceamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UJX5DRdXGBU-"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "# Separar a maioria e minoria\n",
        "df_majority = df[df['Class']==0]\n",
        "df_minority = df[df['Class']==1]\n",
        "# Downsample\n",
        "df_majority_downsampled = resample(df_majority, \n",
        "                                 replace=False,    \n",
        "                                 n_samples=198) # Colocar aqui o downsample que quiser\n",
        "# Combinar a maioria downsampled com a minoria\n",
        "df = pd.concat([df_majority_downsampled, df_minority])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtQIQFFkG4lJ"
      },
      "source": [
        "## Meu código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "PWtRwmzuFzC6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[colunas.drop('Class')]\n",
        "Y = df['Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MU10YneUelL",
        "outputId": "22ba5ef4-6c7f-4e49-c3f4-ecab1cd76b8a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(787, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "VmZRtzvaZYLi"
      },
      "outputs": [],
      "source": [
        "X_test = np.array(X_test)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features, n_samples = X_train.shape\n",
        "weights1 = np.ones(n_samples)\n",
        "weights2 = np.ones(n_samples)\n",
        "hneurons = np.zeros(10)\n",
        "bias1 = np.zeros(n_samples)\n",
        "bias2 = np.zeros(10)\n"
      ],
      "metadata": {
        "id": "9IsM1nnKUMil"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def activation1(x):\n",
        "  return x * (x > 0)\n",
        "\n",
        "def dact1(x):\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "def activation2(x):\n",
        "  resultado = 1/(1 + np.exp(-x))\n",
        "  if resultado >= 0.5:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dact2(x):\n",
        "  return activation2(x) * (1.0 - activation2(x))\n"
      ],
      "metadata": {
        "id": "kbwzBhcFaFXb"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "id": "0pHNOTAQEN7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista = []\n",
        "predictions = []\n",
        "for i in range(len(X_train)):\n",
        "  lista = []\n",
        "  for j in range(len(X_train[i])):\n",
        "    forward = (X_train[i][j] * weights1[j]) + bias1[j]\n",
        "    lista.append(forward)\n",
        "  for k in range(len(hneurons)):\n",
        "    z = np.sum(lista)\n",
        "    ativada = activation1(z)\n",
        "    hneurons[k] = ativada\n",
        "  soma = []\n",
        "  for j in range(len(hneurons)):\n",
        "    forward = hneurons[j] * weights2[j] + bias2[j]\n",
        "    soma.append(forward)\n",
        "    z = np.sum(soma)\n",
        "    ativada_output = activation2(z)\n",
        "  \n",
        "  predictions.append(ativada_output)\n",
        "\n",
        "cost = []\n",
        "for i in range(len(predictions)):\n",
        "  custo = np.square(predictions[i] - y_train[i])\n",
        "  cost.append(custo)\n",
        "  custo_total = np.sum(cost)\n",
        "\n"
      ],
      "metadata": {
        "id": "KmiAL_e8UhUs"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "derw2 = []\n",
        "for i in range(len(hneurons)):\n",
        "\n",
        "  for j in range(len(y_train)):\n",
        "\n",
        "    dw2 = 2 * (predictions[j] - y_train[j]) * dact2(ativada_output) * np.sum(weights1)\n",
        "    derw2.append(dw2)"
      ],
      "metadata": {
        "id": "gpMmF6nYlZn5"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1/(1 + np.exp(-z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlAcpoz_Ezn7",
        "outputId": "0f66a02f-2eda-449c-f899-34b327d50a27"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhHpbriCE32Z",
        "outputId": "4bc8352f-e03c-442a-cac1-d9786deebd25"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(derw2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpw9K0I2EhUa",
        "outputId": "b5710a25-fec8-4297-a164-d06e6cc4d844"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(derw2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VU7l1mKGktE",
        "outputId": "964f9054-163b-4a30-b357-9cad1a42c277"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EquJaDOMeg0W"
      },
      "source": [
        "## Daqui pra baixo são os códigos copiados ruins"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights1 = []\n",
        "weights2 = []\n",
        "bias1 = 0\n",
        "bias2 = 0\n",
        "hidden = np.zeros(10)\n",
        "\n",
        "def forward():\n",
        "  for _ in range(iter):\n",
        "    for idx, x_i in enumerate(X_train):\n",
        "      forward = np.matmul(X_train,weights1) + bias1\n",
        "      forward = activation1(forward)\n",
        "      forward = np.matmul(forward, weights2) + bias2\n",
        "      pred = activation2(forward)\n",
        "\n",
        "      predictions = np.append(predictions, pred)\n",
        "    \n",
        "def backward():\n",
        "  for i in range\n"
      ],
      "metadata": {
        "id": "lfhxB6txMeXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "fsEotdbum_mD",
        "outputId": "ecd7132a-9641-440c-c0d5-02174a2cbf9c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'tuple'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-90e93aeff838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-90e93aeff838>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     33\u001b[0m                   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                   \u001b[0mhneuron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhneuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._unit_step_func\n",
        "        self.weights1 = None\n",
        "        self.weights2 = None\n",
        "        self.bias1 = None\n",
        "        self.bias2 = None\n",
        "        self.hidden = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # init parameters\n",
        "        self.weights1 = np.zeros(n_features)\n",
        "        self.weights2 = np.zeros(10)\n",
        "        self.bias1 = 0\n",
        "        self.bias2 = 0\n",
        "        self.hidden = np.zeros(10)\n",
        "\n",
        "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "\n",
        "            for i in range(len(self.hidden)):\n",
        "\n",
        "                for idx, x_i in enumerate(X):\n",
        "                  \n",
        "                  output = np.dot(x_i, self.weights1) + self.bias1\n",
        "                  hneuron = self.activation_func(output)\n",
        "                  self.hidden[i] = hneuron\n",
        "\n",
        "            for idx, x_ii in enumerate(hidden):\n",
        "                  output = np.dot(x_ii, self.weights2) + self.bias2\n",
        "                  y_predicted = self.activation_func(output)\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "                # Perceptron update rule\n",
        "            ERROR_output = y_predicted - y_train[idx]\n",
        "            DELTA_output = ((-1)*(ERROR_output) * self.deriv_func(self.y_predicted))\n",
        "            update1 = self.lr * (y_[idx] - y_predicted)\n",
        "\n",
        "            self.weights += update * x_i\n",
        "            self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation_func(linear_output)\n",
        "        return y_predicted\n",
        "\n",
        "    def _unit_step_func(self, x):\n",
        "        return np.where(x*(x > 0))\n",
        "\n",
        "    def deriv_func(self, x):\n",
        "        return np.where(1 * (x > 0))\n",
        "\n",
        "\n",
        "# Testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Imports\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn import datasets\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "        return accuracy\n",
        "\n",
        "    X = np.array(df[colunas.drop('Class')])\n",
        "    Y = np.array(df['Class'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, Y, test_size=0.2, random_state=123\n",
        "    )\n",
        "\n",
        "    p = Perceptron(learning_rate=0.01, n_iters=1000)\n",
        "    p.fit(X_train, y_train)\n",
        "    predictions = p.predict(X_test)\n",
        "\n",
        "    print(\"Perceptron classification accuracy\", accuracy(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro teste: Perceptron classification accuracy 0.9985955549313578\n",
        "\n",
        "0.9982\n",
        "\n",
        "array([[56831,    64],\n",
        "       [   16,    51]])\n",
        "\n",
        "0.76119"
      ],
      "metadata": {
        "id": "ir5Q73B-5w34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L6yTU2veKcc",
        "outputId": "eb711608-63b7-4178-f14f-094973b7c9ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[56831,    64],\n",
              "       [   16,    51]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "confusion_matrix(predictions, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
        "import random\n",
        "\n",
        "class MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n",
        "    def __init__(self, params=None):     \n",
        "        if (params == None):\n",
        "            self.inputLayer = 4                        # Input Layer\n",
        "            self.hiddenLayer = 5                       # Hidden Layer\n",
        "            self.outputLayer = 3                       # Outpuy Layer\n",
        "            self.learningRate = 0.005                  # Learning rate\n",
        "            self.max_epochs = 600                      # Epochs\n",
        "            self.iasHiddenValue = -1                   # Bias HiddenLayer\n",
        "            self.BiasOutputValue = -1                  # Bias OutputLayer\n",
        "            self.activation = self.ativacao['sigmoid'] # Activation function\n",
        "            self.deriv = self.derivada['sigmoid']\n",
        "        else:\n",
        "            self.inputLayer = params['InputLayer']\n",
        "            self.hiddenLayer = params['HiddenLayer']\n",
        "            self.OutputLayer = params['OutputLayer']\n",
        "            self.learningRate = params['LearningRate']\n",
        "            self.max_epochs = params['Epocas']\n",
        "            self.BiasHiddenValue = params['BiasHiddenValue']\n",
        "            self.BiasOutputValue = params['BiasOutputValue']\n",
        "            self.activation = self.ativacao[params['ActivationFunction']]\n",
        "            self.deriv = self.derivada[params['ActivationFunction']]\n",
        "        \n",
        "        'Starting Bias and Weights'\n",
        "        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n",
        "        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n",
        "        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n",
        "        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n",
        "        self.classes_number = 3 \n",
        "        \n",
        "    pass\n",
        "    \n",
        "    def starting_weights(self, x, y):\n",
        "        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n",
        "\n",
        "    ativacao = {\n",
        "         'sigmoid': (lambda x: 1/(1 + np.exp(-x))),\n",
        "            'tanh': (lambda x: np.tanh(x)),\n",
        "            'Relu': (lambda x: x*(x > 0)),\n",
        "               }\n",
        "    derivada = {\n",
        "         'sigmoid': (lambda x: x*(1-x)),\n",
        "            'tanh': (lambda x: 1-x**2),\n",
        "            'Relu': (lambda x: 1 * (x>0))\n",
        "               }\n",
        " \n",
        "    def Backpropagation_Algorithm(self, x):\n",
        "        DELTA_output = []\n",
        "        'Stage 1 - Error: OutputLayer'\n",
        "        ERROR_output = self.output - self.OUTPUT_L2\n",
        "        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n",
        "        \n",
        "        arrayStore = []\n",
        "        'Stage 2 - Update weights OutputLayer and HiddenLayer'\n",
        "        for i in range(self.hiddenLayer):\n",
        "            for j in range(self.OutputLayer):\n",
        "                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n",
        "                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n",
        "      \n",
        "        'Stage 3 - Error: HiddenLayer'\n",
        "        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n",
        " \n",
        "        'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\n",
        "        for i in range(self.OutputLayer):\n",
        "            for j in range(self.hiddenLayer):\n",
        "                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))\n",
        "                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n",
        "                \n",
        "    def show_err_graphic(self,v_erro,v_epoca):\n",
        "        plt.figure(figsize=(9,4))\n",
        "        plt.plot(v_epoca, v_erro, \"m-\",color=\"b\", marker=11)\n",
        "        plt.xlabel(\"Number of Epochs\")\n",
        "        plt.ylabel(\"Squared error (MSE) \");\n",
        "        plt.title(\"Error Minimization\")\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self, X, y):\n",
        "        'Returns the predictions for every element of X'\n",
        "        my_predictions = []\n",
        "        'Forward Propagation'\n",
        "        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden\n",
        "        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output\n",
        "                                 \n",
        "        for i in forward:\n",
        "            my_predictions.append(max(enumerate(i), key=lambda x:x[1])[0])\n",
        "            \n",
        "        array_score = []\n",
        "        for i in range(len(my_predictions)):\n",
        "            if my_predictions[i] == 0: \n",
        "                array_score.append([i, 'Iris-setosa', my_predictions[i], y[i]])\n",
        "            elif my_predictions[i] == 1:\n",
        "                 array_score.append([i, 'Iris-versicolour', my_predictions[i], y[i]])\n",
        "            elif my_predictions[i] == 2:\n",
        "                 array_score.append([i, 'Iris-virginica', my_predictions[i], y[i]])\n",
        "                    \n",
        "        dataframe = pd.DataFrame(array_score, columns=['_id', 'class', 'output', 'hoped_output'])\n",
        "        return my_predictions, dataframe\n",
        "\n",
        "    def fit(self, X, y):  \n",
        "        count_epoch = 1\n",
        "        total_error = 0\n",
        "        n = len(X); \n",
        "        epoch_array = []\n",
        "        error_array = []\n",
        "        W0 = []\n",
        "        W1 = []\n",
        "        while(count_epoch <= self.max_epochs):\n",
        "            for idx,inputs in enumerate(X): \n",
        "                self.output = np.zeros(self.classes_number)\n",
        "                'Stage 1 - (Forward Propagation)'\n",
        "                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n",
        "                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n",
        "                'Stage 2 - One-Hot-Encoding'\n",
        "                if(y[idx] == 0): \n",
        "                    self.output = np.array([1,0,0]) #Class1 {1,0,0}\n",
        "                elif(y[idx] == 1):\n",
        "                    self.output = np.array([0,1,0]) #Class2 {0,1,0}\n",
        "\n",
        "                \n",
        "                square_error = 0\n",
        "                for i in range(self.OutputLayer):\n",
        "                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n",
        "                    square_error = (square_error + (0.05 * erro))\n",
        "                    total_error = total_error + square_error\n",
        "         \n",
        "                'Backpropagation : Update Weights'\n",
        "                self.Backpropagation_Algorithm(inputs)\n",
        "                \n",
        "            total_error = (total_error / n)\n",
        "            if((count_epoch % 50 == 0)or(count_epoch == 1)):\n",
        "                print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n",
        "                error_array.append(total_error)\n",
        "                epoch_array.append(count_epoch)\n",
        "                \n",
        "            W0.append(self.WEIGHT_hidden)\n",
        "            W1.append(self.WEIGHT_output)\n",
        "             \n",
        "                \n",
        "            count_epoch += 1\n",
        "        self.show_err_graphic(error_array,epoch_array)\n",
        "        \n",
        "        plt.plot(W0[0])\n",
        "        plt.title('Weight Hidden update during training')\n",
        "        plt.legend(['neuron1', 'neuron2', 'neuron3', 'neuron4', 'neuron5'])\n",
        "        plt.ylabel('Value Weight')\n",
        "        plt.show()\n",
        "        \n",
        "        plt.plot(W1[0])\n",
        "        plt.title('Weight Output update during training')\n",
        "        plt.legend(['neuron1', 'neuron2', 'neuron3'])\n",
        "        plt.ylabel('Value Weight')\n",
        "        plt.show()\n",
        "\n",
        "        return self"
      ],
      "metadata": {
        "id": "mfNbJcmVF1p_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = {'InputLayer':29, 'HiddenLayer':5, 'OutputLayer':1,\n",
        "              'Epocas':700, 'LearningRate':0.005,'BiasHiddenValue':-1, \n",
        "              'BiasOutputValue':-1, 'ActivationFunction':'sigmoid'}\n",
        "\n",
        "Perceptron = MultiLayerPerceptron(dictionary)\n",
        "Perceptron.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "xK2qgZL_mg45",
        "outputId": "5853c266-c9fc-428d-9971-846e38ee032e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-2a7c553ca5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPerceptron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiLayerPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mPerceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-266c7c1e5df6>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;34m'Backpropagation : Update Weights'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBackpropagation_Algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mtotal_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_error\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-266c7c1e5df6>\u001b[0m in \u001b[0;36mBackpropagation_Algorithm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m'Stage 3 - Error: HiddenLayer'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mdelta_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHT_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDELTA_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_L1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.activation_func = self._unit_step_func\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # init parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "\n",
        "            for idx, x_i in enumerate(X):\n",
        "\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.activation_func(linear_output)\n",
        "\n",
        "                # Perceptron update rule\n",
        "                update = self.lr * (y_[idx] - y_predicted)\n",
        "\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation_func(linear_output)\n",
        "        return y_predicted\n",
        "\n",
        "    def _unit_step_func(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "\n",
        "# Testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Imports\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn import datasets\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "        return accuracy\n",
        "\n",
        "    X = df[colunas.drop('Class')]\n",
        "    Y = df['Class']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "    p = Perceptron(learning_rate=0.01, n_iters=1000)\n",
        "    p.fit(X_train, y_train)\n",
        "    predictions = p.predict(X_test)\n",
        "\n",
        "    print(\"Perceptron classification accuracy\", accuracy(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "yLj4oe9xLhbF",
        "outputId": "faa21cb9-7fd1-49f7-99fa-ea61f594b106"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-67e9f03ab0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-67e9f03ab0ea>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mlinear_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U2'), dtype('<U32')) -> None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2ck-HyE8O8l"
      },
      "outputs": [],
      "source": [
        "def relu(z):\n",
        "    return x * (x > 0)\n",
        "def forward_propagation(X, parameters):\n",
        "    \n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.relu(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = relu(Z2)\n",
        "    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n",
        "    \n",
        "    return A2, cache\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyCEaBAgCHXo"
      },
      "outputs": [],
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    #number of training example\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "   \n",
        "    dZ2 = A2-Y\n",
        "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
        "    dW1 = (1/m) * np.dot(dZ1, X.T) \n",
        "    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "    \n",
        "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n",
        "    \n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnt3-cjUxQjd"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "pred = np.array([])\n",
        "k = 5\n",
        "tam = len(distancias)\n",
        "for i in range(tam):\n",
        "  idxkneigh = distancias[i].argsort()[:k] # Pega os indices das k menores distancias\n",
        "  vizinhos = [y_train[idxkneigh[j]] for j in range(k)] # Busca qual a classificação desses indices: red ou não\n",
        "  mode_info = stats.mode(vizinhos) # Pega a moda das classificações dos KNN\n",
        "  pred = np.append(pred, mode_info[0]) # Prediz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z9OC-L6CZoY",
        "outputId": "26c051a7-f4ec-4f83-bc2a-2510a4ed8096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "629\n"
          ]
        }
      ],
      "source": [
        "acertos = 0\n",
        "for i in range(tam):\n",
        "  if pred[i] == y_test[i]:\n",
        "    acertos += 1\n",
        "\n",
        "print(acertos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AExtsXNtvxYE",
        "outputId": "a0f02c86-454a-4f41-d70a-d8eada56e43a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9828125"
            ]
          },
          "execution_count": 239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acertos/(tam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIgrve4Fj_n-"
      },
      "source": [
        "99% Precisão normalizado e com PCA\n",
        "\n",
        "92% Precisão sem normalizar e sem PCA\n",
        "\n",
        "98% Precisão normalizando e sem PCA"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "periodo4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyON719pJpo//NZlakGMrkL+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}